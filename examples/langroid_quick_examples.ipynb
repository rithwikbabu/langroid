{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/langroid/langroid/blob/main/examples/langroid_quick_examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "<img width=\"700\" src=\"https://raw.githubusercontent.com/langroid/langroid/main/docs/assets/langroid-card-lambda-ossem-rust-1200-630.png\" alt=\"Langroid\">\n",
    "\n",
    "# Overview\n",
    "\n",
    "This notebook provides the runnable code for the six [**Usage Examples**](https://github.com/langroid/langroid#tada-usage-examples) described in [Langroid repo](https://github.com/langroid/langroid).\n",
    "\n",
    "**NOTE:** Notebooks (colab, jupyter, or otherwise) are *not* an ideal way to run interactive chat loops. We are showing these examples here since we recognize that Colab notebooks offer the benefit of having a ready to run environment with minimal setup. But we encourage you to try the python scripts in the [examples folder](https://github.com/langroid/langroid/tree/main/examples) of the repo on the command line for the best experience.\n",
    "\n",
    "In the first two cells we show the steps for setting up the requirements to run the examples including the installation of `Langroid` package and setting up the `OPENAI_API_KEY`.\n"
   ],
   "metadata": {
    "id": "uIV7QkkrC8O7"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install Langroid\n",
    "\n",
    "At the end there may be a message saying \"RESTART RUNTIME\", which can be safely ignored."
   ],
   "metadata": {
    "id": "hoTp_cNcriIg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install langroid"
   ],
   "metadata": {
    "id": "PYaFworprwEJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up `OPENAI_API_KEY`\n",
    "\n",
    "This code will ask the user to provide the `OPENAI_API_KEY`. Before running this cell, please follow these steps to get the key.\n",
    "Login to your OpenAI account --> go to `View API Keys` from the drop-down list on the top-right corner --> click on the botton **create new secret key** --> a new screen will pop up --> press the botton **create secret key**.\n",
    "\n",
    "Visit [this page](https://help.openai.com/en/articles/4936850-where-do-i-find-my-secret-api-key) for more info about where to find the API Key."
   ],
   "metadata": {
    "id": "v-BiRXu9JQ5H"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.ERROR)\n",
    "\n",
    "\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = getpass('Enter your OPENAI_API_KEY key: ')"
   ],
   "metadata": {
    "id": "GOR8OsfvsN2k"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Now you can can try any of the following examples. It is recommended to go through these in sequence, although the order does NOT matter.**\n",
    "\n",
    "---"
   ],
   "metadata": {
    "id": "Tw5RzVKl3pUr"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Direct interaction with OpenAI LLM\n",
    "\n",
    "In this simple example, we are directly sending a message-sequence to the OpenAI `chatCompletion` API. Note that to have a multi-round converation we have to manually accumulate the dialog.\n",
    "\n",
    "First do various imports."
   ],
   "metadata": {
    "id": "5rBWNOuXEygx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langroid.language_models.base import LLMMessage, Role\n",
    "from langroid.language_models.openai_gpt import (\n",
    "        OpenAIGPTConfig,\n",
    "        OpenAIChatModel,\n",
    "        OpenAIGPT,\n",
    ")\n",
    "from langroid.language_models.base import LLMMessage, Role"
   ],
   "metadata": {
    "id": "lGY2XyHyD0oJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can use the API `OpenAIGPTConfig` to set the configurations of the OpenAI LLM model. We then define the LLM model using `OpenAIGPT`. We can also Specify the messages that will be sent to instruct the model.\n",
    "`Langroid` supports various roles provided by OpenAI.\n"
   ],
   "metadata": {
    "id": "bMR6Dani_l9C"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "cfg = OpenAIGPTConfig(chat_model=OpenAIChatModel.GPT4) # or GPT_3_5_TURBO\n",
    "\n",
    "mdl = OpenAIGPT(cfg)\n",
    "\n",
    "messages = [\n",
    "  LLMMessage(content=\"You are a helpful assistant\",  role=Role.SYSTEM),\n",
    "  LLMMessage(content=\"What is the capital of Ontario?\",  role=Role.USER),\n",
    "]\n",
    "\n",
    "response = mdl.chat(messages, max_tokens=200)\n",
    "print(\"LLM response is:: \", response.message)\n",
    "\n",
    "# accumulate messages manually\n",
    "\n",
    "messages.append(response.to_LLMMessage())\n",
    "messages.append(LLMMessage(content=\"what about India?\", role=Role.USER))\n",
    "response = mdl.chat(messages, max_tokens=200)\n",
    "print(\"LLM response is:\", response.message)"
   ],
   "metadata": {
    "id": "JPul8c4uD1kH"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above is a \"raw\" LLM interaction where you have to manage\n",
    "message history. Using an Agent to wrap an LLM, and wrapping an Agent in a Task, we can set up an interactive, multi-round chat much more easily, as we show next."
   ],
   "metadata": {
    "id": "3HzZhfyAfRCQ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A note on the rest of the examples\n",
    "In the interactive examples below, the conversation loop pauses for human input: in most cases you would hit enter (unless the example requires you to ask a question).\n",
    "The interaction looks much better when run on a terminal,\n",
    "and a notebook is not ideal for these. However we realize a Colab notebook does offer the benefit of having a ready to run environment."
   ],
   "metadata": {
    "id": "4TjgYEfK34NZ"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define an agent, set up a task, and run it\n",
    "\n",
    "Say you want to have a multi-round interactive chat with an LLM.\n",
    "\n",
    "`Langroid` simplifies this process. We just need to create a `ChatAgent`, wrap it in a `Task`, and finally run the task."
   ],
   "metadata": {
    "id": "OaIiDn8zOurc"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langroid.agent.chat_agent import ChatAgent, ChatAgentConfig\n",
    "from langroid.agent.task import Task\n",
    "from langroid.language_models.openai_gpt import OpenAIChatModel, OpenAIGPTConfig"
   ],
   "metadata": {
    "id": "wFX7u9k-OyDw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "For constructing the `ChatAgent`, we need to set up some configurations:\n",
    "\n",
    "*   No vector database will be created\n",
    "*   chat model (`GPT4` or `GPT_3_5_TURBO`)\n",
    "\n",
    "Note that `Langroid` offers specialized chatting agents such as `DocChatAgent` and `TableChatAgent`, which we will see later."
   ],
   "metadata": {
    "id": "vaXKQWh-SBgH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "config = ChatAgentConfig(\n",
    "    llm = OpenAIGPTConfig(\n",
    "        chat_model=OpenAIChatModel.GPT4,\n",
    "    ),\n",
    "    vecdb=None,\n",
    ")\n",
    "agent = ChatAgent(config)"
   ],
   "metadata": {
    "id": "PblTvXC6QZsS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "A `ChatAgent` by itself offers 3 standard \"responders\": the LLM, the human User, and the Agent itself (e.g. to handle tool/function-calling by the LLM). To use these responders in an interactive loop, we need to wrap the Agent in a task,\n",
    "and call its `run()` method.\n",
    "\n",
    "A prompt will be displayed after running this task, so you can interact with the `ChatAgent`.\n",
    "\n",
    "Type your questions and the agent will provide the LLM responses. When done, type `q` to exit.\n"
   ],
   "metadata": {
    "id": "LO8O-7vZSoS2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "agent.message_history.clear()\n",
    "task = Task(agent, name=\"Bot\")\n",
    "task.set_color_log(enable=False)\n",
    "task.run()"
   ],
   "metadata": {
    "id": "wheZ7NJGO2X4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Three communicating agents\n",
    "\n",
    "The above example involved a single `ChatAgent`, but in non-trivial applications, we will often find it easier to divide responsibilities among multiple agents, each with different skills and responsibilities.\n",
    "\n",
    "If you attempt to solve these with a single Agent, you would have to keep track of multiple conversation states and loops, and it quickly gets out of hand. Agents offer a way to solve complex tasks in a modular fashion. Moreover, specialized agents can be designed and tested in isolation, and then combined to solve various tasks.\n",
    "\n",
    "`Langroid` streamlines the process of setting up multiple agents and orchestrating their interaction. Here's a toy numerical example (this helps keep token costs low!). Imagine a task where we want to construct a series of numbers using the following rule to transform the current number $n$:\n",
    "- if $n$ is even, the next number is $n/2$\n",
    "- if $n$ is odd, the next number is $3n+1$.\n",
    "\n",
    "We can have 3 agents collaborate to produce this sequence.\n",
    "Given the current number $n$,\n",
    "- `repeater_agent` simply returns $n$,\n",
    "- `even_agent` specializes in handling even numbers, and returns $n/2$ if $n$ is even, else says \"DO-NOT-KNOW\"\n",
    "- `odd_agent` specializes in handling odd numbers, and returns $3*n+1$ if $n$ is odd, else says \"DO-NOT-KNOW\""
   ],
   "metadata": {
    "id": "B4q8ojyAQ_OV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langroid.utils.constants import NO_ANSWER\n",
    "from langroid.agent.chat_agent import ChatAgent, ChatAgentConfig\n",
    "from langroid.agent.task import Task\n",
    "from langroid.language_models.openai_gpt import OpenAIChatModel, OpenAIGPTConfig"
   ],
   "metadata": {
    "id": "oUbVybSuRIFX"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "As before, we define chat model that will be used by the agents:"
   ],
   "metadata": {
    "id": "_1aeRre35Ghd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "config = ChatAgentConfig(\n",
    "    llm = OpenAIGPTConfig(\n",
    "        chat_model=OpenAIChatModel.GPT4,\n",
    "    ),\n",
    "    vecdb = None,\n",
    ")"
   ],
   "metadata": {
    "id": "tAQIs76rRaF4"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we create the `repeater_agent` and define its corresponding task, which comprises the following settings:\n",
    "\n",
    "\n",
    "*   **Name**: name of the agent\n",
    "*   **llm_delegate**: whether to delegate control to LLM; conceptually, the \"controlling entity\" is the one \"seeking\" responses to its queries, and has a goal it is aiming to achieve. The \"controlling entity\" is either the LLM or the USER. (Note within a Task there is just one LLM, and all other entities are proxies of the \"User\" entity).\n",
    "*   **single_round**: If true, the task runs until one message by the controller and a subsequent response by the non-controller. If false, runs for the specified number of turns in `run`, or until `done()` is true.\n",
    "* **system_message**: provides instructions to the LLM."
   ],
   "metadata": {
    "id": "xpqzS-ozUowm"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "repeater_agent = ChatAgent(config)\n",
    "repeater_task = Task(\n",
    "    repeater_agent,\n",
    "    name = \"Repeater\",\n",
    "    system_message=\"\"\"\n",
    "    Your job is to repeat whatever number you receive.\n",
    "    \"\"\",\n",
    "    llm_delegate=True, # LLM takes charge of task\n",
    "    single_round=False,\n",
    ")"
   ],
   "metadata": {
    "id": "AI5oFOM2SLgM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we define our second agent `even_agent` and its task `even_task`. Notice it takes the same `config` that we previously created."
   ],
   "metadata": {
    "id": "zVJ-nbs08Ub5"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "even_agent = ChatAgent(config)\n",
    "even_task = Task(\n",
    "    even_agent,\n",
    "    name = \"EvenHandler\",\n",
    "    system_message=f\"\"\"\n",
    "    You will be given a number.\n",
    "    If it is even, divide by 2 and say the result, nothing else.\n",
    "    If it is odd, say {NO_ANSWER}\n",
    "    \"\"\",\n",
    "    single_round=True,  # task done after 1 step() with valid response\n",
    ")"
   ],
   "metadata": {
    "id": "pgOFydVqRbPc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we create the 3rd agent `odd_agent` and its task `odd_task`.\n"
   ],
   "metadata": {
    "id": "z-RAYL9S9XTM"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "odd_agent = ChatAgent(config)\n",
    "odd_task = Task(\n",
    "    odd_agent,\n",
    "    name = \"OddHandler\",\n",
    "    system_message=f\"\"\"\n",
    "    You will be given a number n.\n",
    "    If it is odd, return (n*3+1), say nothing else.\n",
    "    If it is even, say {NO_ANSWER}\n",
    "    \"\"\",\n",
    "    single_round=True,  # task done after 1 step() with valid response\n",
    ")"
   ],
   "metadata": {
    "id": "ukf79CqIScHQ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We use `add_sub_task` to orchestrate the collaboration between the agents.  Specifically, `repeater_task` will act as the \"main\", and we add `even_task` and `odd_task` as\n",
    "subtasks. For more details see these [docs](https://langroid.github.io/langroid/quick-start/multi-agent-task-delegation/#task-collaboration-via-sub-tasks).\n",
    "\n",
    "\n",
    "Finally, we kickoff the task with a starting number 3, using `repeater_task.run(\"3\")`.\n",
    "\n",
    "Remember to keep hitting enter when it's the human's turn, and hit \"q\" to end the conversation."
   ],
   "metadata": {
    "id": "PbGYXCpR9uVH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "repeater_task.add_sub_task([even_task, odd_task])\n",
    "repeater_task.set_color_log(enable=False)\n",
    "repeater_task.run(\"3\")"
   ],
   "metadata": {
    "id": "0vy77Os8TAas"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Simple Tool/Function-calling example\n",
    "\n",
    "Here is a simple numerical example showcasing how `Langroid` supports tools/function-calling. For more details see these [doc pages](https://langroid.github.io/langroid/quick-start/chat-agent-tool/)\n",
    "\n",
    "Say the agent has a secret list of numbers, and we want the LLM to find the smallest number in the list. We want to give the LLM the ability to use a **probe** tool/function which takes a single number `n` as an argument. The tool handler method in the agent returns how many numbers in its list are at most `n`."
   ],
   "metadata": {
    "id": "t7c3qKvcTwTG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "from langroid.agent.tool_message import ToolMessage\n",
    "from langroid.agent.chat_agent import ChatAgent, ChatAgentConfig\n",
    "from langroid.language_models.openai_gpt import OpenAIChatModel, OpenAIGPTConfig\n",
    "from langroid.agent.task import Task\n",
    "from langroid.utils.configuration import set_global, Settings\n",
    "from langroid.embedding_models.models import OpenAIEmbeddingsConfig"
   ],
   "metadata": {
    "id": "qMG5Jq7wUZRJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "To use tools/function-calling in `Langroid`, we first **define** the tool as a subclass of `ToolMessage` to specify some details about the tool (e.g., name and parameters) and when it can be used/triggered:\n",
    "* **request**: is the name of the tool/function, as well as the name of the Agent method that \"handles\" the tool.\n",
    "* **purpose**: general description to give hints to LLM when this tool can be used\n",
    "* **number**: is a function-argument for the `probe` tool and its type is `int`"
   ],
   "metadata": {
    "id": "htqVMFU2pog6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class ProbeTool(ToolMessage):\n",
    "  request: str = \"probe\"\n",
    "  purpose: str = \"\"\"\n",
    "        To find how many numbers in my list are less than or equal to\n",
    "        the <number> you specify.\n",
    "        \"\"\" # note  <number> corresponds to the name of the tool's argument/parameter\n",
    "  number: int\n"
   ],
   "metadata": {
    "id": "3eIidMmZUgir"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we create an agent `SpyGameAgent`, with a special method `probe` to handle the `probe` tool/function.\n",
    "Notice the argument of the `probe` method is an instance of the class `ProbeTool` that we created in the previous step."
   ],
   "metadata": {
    "id": "vBAw8kxkqa42"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class SpyGameAgent(ChatAgent):\n",
    "  def __init__(self, config: ChatAgentConfig):\n",
    "    super().__init__(config)\n",
    "    self.numbers = [3, 4, 8, 11, 15, 25, 40, 80, 90] # agent's secret list\n",
    "\n",
    "  def probe(self, msg: ProbeTool) -> str:\n",
    "    # return how many numbers in self.numbers are less or equal to msg.number\n",
    "    return str(len([n for n in self.numbers if n <= msg.number]))"
   ],
   "metadata": {
    "id": "einGIzjQUyX7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we instantiate the `SpyGameAgent` as an object `spy_game_agent`, and \"associate\" the `probe` tool with this agent, using the `enable_message` method of the `ChatAgent`.  We then wrap the `spy_game_agent` in a `Task` object, with instructions (`system_message`) on what it should aim for."
   ],
   "metadata": {
    "id": "pG2MJJN4yCHb"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "spy_game_agent = SpyGameAgent(\n",
    "    ChatAgentConfig(\n",
    "        llm = OpenAIGPTConfig(\n",
    "            chat_model=OpenAIChatModel.GPT4,\n",
    "        ),\n",
    "        vecdb=None, # no vector database needed\n",
    "    )\n",
    ")\n",
    "\n",
    "spy_game_agent.enable_message(ProbeTool)\n",
    "\n",
    "task = Task(\n",
    "        spy_game_agent,\n",
    "        name=\"Spy\",\n",
    "        system_message=\"\"\"\n",
    "            I have a list of numbers between 1 and 20.\n",
    "            Your job is to find the smallest of them.\n",
    "            To help with this, you can give me a number and I will\n",
    "            tell you how many of my numbers are equal or less than your number.\n",
    "            Once you have found the smallest number,\n",
    "            you can say DONE and report your answer.\n",
    "        \"\"\",\n",
    "    )"
   ],
   "metadata": {
    "id": "OM6Lk3uWVOCB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now run the task.\n",
    "\n",
    "Remember to keep hitting enter when it's the human's turn, and hit \"q\" to end the conversation."
   ],
   "metadata": {
    "id": "aQ93n0kA_rM0"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "spy_game_agent.message_history.clear()\n",
    "task.set_color_log(enable=False)\n",
    "task.run()"
   ],
   "metadata": {
    "id": "xAVoOcgsNWOv"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Chat with documents (file paths, URLs, etc)\n",
    "\n",
    "In the previous examples, the Agents did not use any external documents. In this example, we we set up an Agent that supports \"chatting\" with documents. Specifically, we use the `DocChatAgent` class to ask questions about a set of URLs.\n",
    "The `DocChatAgent` first ingests the contents of the websites specified by the URLs by chunking, embedding and indexing them into a vector database (`qdrant` by default). We then wrap the agent in a task and run it interactively.\n",
    "The user can ask questions and the LLM of the agent returns answers using Retrieval Augment Generation, with Evidence Citation.\n"
   ],
   "metadata": {
    "id": "Qmeh3zJTZeL1"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langroid.agent.special.doc_chat_agent import DocChatAgentConfig, DocChatAgent\n",
    "from langroid.language_models.openai_gpt import OpenAIChatModel, OpenAIGPTConfig\n",
    "from langroid.vector_store.qdrantdb import QdrantDBConfig\n",
    "from langroid.embedding_models.models import OpenAIEmbeddingsConfig\n",
    "from langroid.agent.task import Task"
   ],
   "metadata": {
    "id": "qMuSX-DjZjwS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we define the configuration of the `DocChatAgent`. The configurations include the path to access the documents, chat model settings, and vector-DB settings."
   ],
   "metadata": {
    "id": "w1ZcFRJu5K5D"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "config = DocChatAgentConfig(\n",
    "  doc_paths = [\n",
    "    \"https://en.wikipedia.org/wiki/Language_model\",\n",
    "    \"https://en.wikipedia.org/wiki/N-gram_language_model\",\n",
    "  ],\n",
    "  llm = OpenAIGPTConfig(\n",
    "    chat_model=OpenAIChatModel.GPT4,\n",
    "  ),\n",
    "  vecdb=QdrantDBConfig(\n",
    "                collection_name=\"docqa-chat-multi-extract\",\n",
    "                storage_path=\".qdrant/test1/\", # CHANGE THIS PATH IF YOU GET AN ERROR WHEN RE-RUNNING THE CELL\n",
    "        ),\n",
    ")\n",
    "\n",
    "agent = DocChatAgent(config)"
   ],
   "metadata": {
    "id": "tVEHeg9jZpl7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "As before, we wrap the agent in a task, and run it.\n",
    "\n",
    "Remember to keep hitting enter when it's the human's turn, and hit \"q\" to end the conversation."
   ],
   "metadata": {
    "id": "UZyTx3vQN_I4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "agent.message_history.clear()\n",
    "task = Task(agent)\n",
    "task.set_color_log(enable=False)\n",
    "task.run()"
   ],
   "metadata": {
    "id": "OMFDp8WxaAI3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tool/Function-calling to extract structured information from text\n",
    "\n",
    "Let's combine multi-agent interaction, Retrieval-Augmented Generation, and tools/function-calling, for a more realistic example. Suppose you want an agent to extract the key terms of a lease, from a lease document, as a nested JSON structure.\n",
    "This can be accomplished by instructing the LLM to use a specific tool.\n",
    "\n",
    "To simplify the solution, we separate the skills/responsibilities into two different Agents:\n",
    "- `LeaseExtractorAgent` has no access to the lease, and is responsible for gathering the key terms into a specific structured form\n",
    "- `DocChatAgent` has access to the lease and answers specific questions it receives from the `LeaseExtractorAgent`.\n"
   ],
   "metadata": {
    "id": "kZqX1J6qWhHk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from rich import print\n",
    "from pydantic import BaseModel, BaseSettings\n",
    "from typing import List\n",
    "import json\n",
    "\n",
    "from langroid.agent.special.doc_chat_agent import DocChatAgent, DocChatAgentConfig\n",
    "from langroid.agent.chat_agent import ChatAgent, ChatAgentConfig\n",
    "from langroid.vector_store.qdrantdb import QdrantDBConfig\n",
    "from langroid.agent.task import Task\n",
    "from langroid.agent.tool_message import ToolMessage\n",
    "from langroid.language_models.openai_gpt import OpenAIGPTConfig\n",
    "from langroid.utils.configuration import set_global, Settings\n",
    "from langroid.utils.logging import setup_colored_logging\n",
    "from langroid.utils.constants import NO_ANSWER\n",
    "from langroid.embedding_models.models import OpenAIEmbeddingsConfig"
   ],
   "metadata": {
    "id": "sKRoGAvqWzMx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we define the desired structure of the lease information via Pydantic models. The desired format is a nested JSON structure, which maps to a nested class structure:\n"
   ],
   "metadata": {
    "id": "d-CkI7wk5A8l"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class LeasePeriod(BaseModel):\n",
    "    start_date: str\n",
    "    end_date: str\n",
    "\n",
    "class LeaseFinancials(BaseModel):\n",
    "    monthly_rent: str\n",
    "    deposit: str\n",
    "\n",
    "class Lease(BaseModel):\n",
    "    \"\"\"\n",
    "    Various lease terms.\n",
    "    Nested fields to make this more interesting/realistic\n",
    "    \"\"\"\n",
    "\n",
    "    period: LeasePeriod\n",
    "    financials: LeaseFinancials\n",
    "    address: str"
   ],
   "metadata": {
    "id": "0p8iveEcX_1E"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We then define the `LeaseMessage` tool as a subclass of Langroid's `ToolMessage`. The `LeaseMessage` class has a\n",
    "required argument `terms` of type `Lease`. The `classmethod` named `examples` is used to generate $k$-shot examples for the LLM when instructing it to extract information in the desired structured form (see a later cell below).\n"
   ],
   "metadata": {
    "id": "WcUQylLu5HFh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class LeaseMessage(ToolMessage):\n",
    "    request: str = \"lease_info\" # maps to method of LeaseExtractorAgent\n",
    "    purpose: str = \"\"\"\n",
    "        Collect information about a Commercial Lease.\n",
    "        \"\"\"\n",
    "    terms: Lease\n",
    "\n",
    "    @classmethod\n",
    "    def examples(cls) -> List[\"LeaseMessage\"]:\n",
    "        return [\n",
    "            cls(\n",
    "                terms=Lease(\n",
    "                    period=LeasePeriod(start_date=\"2021-01-01\", end_date=\"2021-12-31\"),\n",
    "                    financials=LeaseFinancials(monthly_rent=\"$1000\", deposit=\"$1000\"),\n",
    "                    address=\"123 Main St, San Francisco, CA 94105\",\n",
    "                ),\n",
    "                result=\"\",\n",
    "            ),\n",
    "        ]\n",
    "\n"
   ],
   "metadata": {
    "id": "XFVCpL8jW7C7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we define the `LeaseExtractorAgent` and add a method `least_info` to handle the tool/function-call `lease_info` defined in the tool `LeaseMessage`. In this case the handling is trivial: if the method receives a valid object of class `LeaseMessage`, it declares \"success\"."
   ],
   "metadata": {
    "id": "Lu03iGEaW0Ur"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class LeaseExtractorAgent(ChatAgent):\n",
    "    def __init__(self, config: ChatAgentConfig):\n",
    "        super().__init__(config)\n",
    "\n",
    "    def lease_info(self, message: LeaseMessage) -> str:\n",
    "        print(\n",
    "            f\"\"\"\n",
    "        DONE! Successfully extracted Lease Info:\n",
    "        {message.terms}\n",
    "        \"\"\"\n",
    "        )\n",
    "        return json.dumps(message.terms.dict())"
   ],
   "metadata": {
    "id": "ZlZ0UtqEXGdz"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Obtain the lease.txt document that we want to parsed\n",
    "!wget https://github.com/langroid/langroid-examples/blob/main/examples/docqa/lease.txt"
   ],
   "metadata": {
    "id": "HtHXm0-UuBRU"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, set up an instance of `DocChatAgent`, point it to the lease document, equip it with a vector database, and instructions on how to answer questions based on extracts retrieved from the vector-store.\n"
   ],
   "metadata": {
    "id": "mdkxu37f7JuK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "doc_agent = DocChatAgent(\n",
    "        DocChatAgentConfig(\n",
    "            doc_paths = [\"lease.txt\"],\n",
    "            vecdb=QdrantDBConfig(\n",
    "                collection_name=\"docqa-chat-multi-extract\",\n",
    "                storage_path=\".data1/data1/\", # CHANGE PATH IF ERROR\n",
    "              ),\n",
    "            summarize_prompt= f\"\"\"\n",
    "                Use the provided extracts to answer the question.\n",
    "                If there's not enough information, respond with {NO_ANSWER}. Use only the\n",
    "                information in these extracts, even if your answer is factually incorrect,\n",
    "                and even if the answer contradicts other parts of the document. The only\n",
    "                important thing is that your answer is consistent with and supported by the\n",
    "                extracts. Compose your complete answer and cite all supporting sources on a\n",
    "                separate separate line as \"EXTRACTS:\".\n",
    "                Show each EXTRACT very COMPACTLY, i.e. only show a few words from\n",
    "                the start and end of the extract, for example:\n",
    "                EXTRACT: \"The world war started in ... Germany Surrendered\"\n",
    "                {{extracts}}\n",
    "                {{question}}\n",
    "                Answer:\n",
    "            \"\"\"\n",
    "        )\n",
    "    )"
   ],
   "metadata": {
    "id": "d8ynRPp6XUvV"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next we wrap the `doc_agent` into a Task, with instructions on its role.\n"
   ],
   "metadata": {
    "id": "DISYqYVr8BQx"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "doc_task = Task(\n",
    "    doc_agent,\n",
    "    name=\"DocAgent\",\n",
    "    llm_delegate=False,\n",
    "    single_round=True,\n",
    "    system_message=\"\"\"You are an expert on Commercial Leases.\n",
    "    You will receive various questions about a Commercial\n",
    "    Lease contract, and your job is to answer them concisely in at most 2 sentences.\n",
    "    Please SUPPORT your answer with an actual EXTRACT from the lease,\n",
    "    showing only a few words from the  START and END of the extract.\n",
    "    \"\"\",\n",
    ")"
   ],
   "metadata": {
    "id": "NBLp0AEy74N-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we instantiate the `lease_extractor_agent`, enable it to use and handle the `LeaseMessage` tool. Then we wrap the `lease_extractor_agent` into a Task, instructing it to gather information in the desired format, by asking questions one at a time. Note how the instruction contains `LeaseMessage.usage_example()`: this example is constructed from the `examples` classmethod above when the `LeaseMessage` was defined.\n"
   ],
   "metadata": {
    "id": "ZhRfOPiE9W-n"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "lease_extractor_agent = LeaseExtractorAgent(\n",
    "    ChatAgentConfig(\n",
    "        vecdb=None,\n",
    "        llm=OpenAIGPTConfig(),\n",
    "    )\n",
    ")\n",
    "\n",
    "lease_extractor_agent.enable_message(\n",
    "    LeaseMessage,\n",
    "    use=True,\n",
    "    handle=True,\n",
    "    force=False,\n",
    ")\n",
    "\n",
    "lease_task = Task(\n",
    "    lease_extractor_agent,\n",
    "    name=\"LeaseExtractorAgent\",\n",
    "    llm_delegate=True,\n",
    "    single_round=False,\n",
    "    system_message=f\"\"\"\n",
    "    You have to collect some information about a Commercial Lease, but you do not\n",
    "    have access to the lease itself.\n",
    "    You can ask me questions about the lease, ONE AT A TIME, I will answer each\n",
    "    question. You only need to collect info corresponding to the fields in this\n",
    "    example:\n",
    "    {LeaseMessage.usage_example()}\n",
    "    If some info cannot be found, fill in {NO_ANSWER}.\n",
    "    When you have collected this info, present it to me using the\n",
    "    'lease_info' function/tool.\n",
    "    \"\"\",\n",
    ")"
   ],
   "metadata": {
    "id": "VL2RYZUX7393"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we set up the `doc_task` as a subtask of the `lease_task` so that the `doc_agent` can respond to questions from the `lease_extractor_agent`.\n",
    " Now, the `lease_extractor_agent` will be asking questions about the lease and `doc_task` will provide the answers, citing evidence extracted from the lease. Once `lease_extractor_agent` collects all the terms of the lease as instructed, it will use the tool `LeaseMessage` to return this information.\n",
    "\n",
    " The next cell runs the `lease_task`. Remember to keep hitting enter when it's the human's turn, and hit \"q\" to end the conversation."
   ],
   "metadata": {
    "id": "XBdAcarpwnYU"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "lease_extractor_agent.message_history.clear()\n",
    "lease_task.add_sub_task(doc_task)\n",
    "lease_task.set_color_log(enable=False)\n",
    "lease_task.run()"
   ],
   "metadata": {
    "id": "1Lt9FhjTFlh2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Chat with tabular data (file paths, URLs, dataframes)\n",
    "\n",
    "Here is how `Langroid's` `TableChatAgent` can be used to chat with tabular data, which can be specified as a URL, file path or Pandas dataframe.\n",
    "\n",
    "The Agent's LLM generates Pandas code to answer the query, via function-calling (or tool/plugin), and the Agent's function-handling method executes the code and returns the answer"
   ],
   "metadata": {
    "id": "uWcH7HoFc-am"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from langroid.agent.special.table_chat_agent import TableChatAgent, TableChatAgentConfig\n",
    "from langroid.agent.task import Task\n",
    "from langroid.language_models.openai_gpt import OpenAIChatModel, OpenAIGPTConfig\n"
   ],
   "metadata": {
    "id": "jS06lshgdBv7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set up a `TableChatAgent` for a data file, URL or dataframe (Ensure the data table has a header row; the delimiter/separator is auto-detected):"
   ],
   "metadata": {
    "id": "tS8pDD2MdgPi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dataset =  \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "# or dataset = \"/path/to/my/data.csv\"\n",
    "# or dataset = pd.read_csv(\"/path/to/my/data.csv\")\n",
    "agent = TableChatAgent(\n",
    "    config=TableChatAgentConfig(\n",
    "        data=dataset,\n",
    "        llm=OpenAIGPTConfig(\n",
    "            chat_model=OpenAIChatModel.GPT4,\n",
    "        ),\n",
    "    )\n",
    ")"
   ],
   "metadata": {
    "id": "qDbA4VGmda10"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, let's set up a task and run it in an interactive loop with the user:\n",
    "Based on `dataset`, you can ask the following question in the prompt:\n",
    "\n",
    "```\n",
    "What is the average alcohol content of wines with a quality rating above 7?\n",
    "```\n",
    "\n",
    "Remember to keep hitting enter when it's the human's turn, and hit \"q\" to end the conversation."
   ],
   "metadata": {
    "id": "em2hcy2Qd67T"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "agent.message_history.clear()\n",
    "task = Task(agent, name=\"DataAssistant\")\n",
    "task.set_color_log(enable=False)\n",
    "task.run()"
   ],
   "metadata": {
    "id": "nhDMm5ndd93W"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
